"""
Layer 3: DOCUMENT RETRIEVAL LAYER
company_document_retriever ì‹¤í–‰í•˜ì—¬ ê´€ë ¨ ë¬¸ì„œ ì²­í¬ë¥¼ ì¶”ì¶œí•˜ëŠ” ë ˆì´ì–´
"""
import re
import os
from typing import List, Dict, Any, Optional
from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from models import DocumentChunk, PipelineContext, EvaluationType
from config import get_config

class CompanyDocumentRetriever:
    """íšŒì‚¬ë³„ ë¬¸ì„œ ê²€ìƒ‰ ë° í•„í„°ë§"""

    def __init__(self):
        self.config = get_config()
        self.llm = ChatOpenAI(
            openai_api_key=self.config["model"].openai_api_key,
            temperature=0.1,
            model=self.config["model"].model_name  # ì˜ˆ: "gpt-4o-mini"
        )

        # ë¬¸ì„œ ê´€ë ¨ì„± í‰ê°€ í”„ë¡¬í”„íŠ¸
        self.relevance_prompt = PromptTemplate(
            input_variables=["company_name", "evaluation_type", "document_content"],
            template="""ë‹¤ìŒ ë¬¸ì„œê°€ {company_name}ì˜ {evaluation_type}ì— ì–¼ë§ˆë‚˜ ê´€ë ¨ì´ ìˆëŠ”ì§€ 0-10ì ìœ¼ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”.

íšŒì‚¬ëª…: {company_name}
í‰ê°€ ìœ í˜•: {evaluation_type}

ë¬¸ì„œ ë‚´ìš©:
{document_content}

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
{{
    "relevance_score": 8.5,
    "key_points": ["í•µì‹¬ í¬ì¸íŠ¸ 1", "í•µì‹¬ í¬ì¸íŠ¸ 2"],
    "section_type": "ì¬ë¬´ì •ë³´|ì‹œì¥ë¶„ì„|ê¸°ìˆ ì •ë³´|íŒ€ì •ë³´|ê¸°íƒ€",
    "reasoning": "ê´€ë ¨ì„± í‰ê°€ ì´ìœ "
}}"""
        )

    def filter_documents_by_company(
        self,
        documents: List[DocumentChunk],
        company_name: str
    ) -> List[DocumentChunk]:
        """íšŒì‚¬ëª…ìœ¼ë¡œ ë¬¸ì„œ í•„í„°ë§"""
        filtered_docs = []

        # íšŒì‚¬ëª… ë³€í˜•ë“¤ ìƒì„±
        company_variations = self._generate_company_variations(company_name)

        for doc in documents:
            # ë©”íƒ€ë°ì´í„°ì—ì„œ íšŒì‚¬ëª… í™•ì¸
            if self._check_company_in_metadata(doc.metadata, company_variations):
                filtered_docs.append(doc)
                continue

            # ë¬¸ì„œ ë‚´ìš©ì—ì„œ íšŒì‚¬ëª… í™•ì¸
            if self._check_company_in_content(doc.content, company_variations):
                filtered_docs.append(doc)
                continue

        return filtered_docs

    def _generate_company_variations(self, company_name: str) -> List[str]:
        """íšŒì‚¬ëª…ì˜ ë‹¤ì–‘í•œ ë³€í˜• ìƒì„±"""
        variations = [company_name]

        # ê¸°ë³¸ ë³€í˜•
        variations.extend([
            company_name + "ãˆœ",
            company_name + " ì£¼ì‹íšŒì‚¬",
            company_name + "ì½”í¼ë ˆì´ì…˜",
            company_name + " Corp",
            company_name + " Inc",
            "ãˆœ" + company_name
        ])

        # ì˜ë¬¸/í•œê¸€ ë³€í™˜ (ì˜ˆì‹œ)
        company_mappings = {
            "í† ìŠ¤": ["Toss", "ë¹„ë°”ë¦¬í¼ë¸”ë¦¬ì¹´", "Viva Republica"],
            "ì¹´ì¹´ì˜¤": ["Kakao", "Daum Kakao"],
            "ë°°ë‹¬ì˜ë¯¼ì¡±": ["ë°°ë¯¼", "ìš°ì•„í•œí˜•ì œë“¤", "Woowa Brothers"],
            "ë„¤ì´ë²„": ["Naver", "NHN"],
            "ì¿ íŒ¡": ["Coupang"],
            "ë‹¹ê·¼ë§ˆì¼“": ["ë‹¹ê·¼", "Daangn"]
        }

        if company_name in company_mappings:
            variations.extend(company_mappings[company_name])

        return list(set(variations))  # ì¤‘ë³µ ì œê±°

    def _check_company_in_metadata(self, metadata: Dict[str, Any], variations: List[str]) -> bool:
        """ë©”íƒ€ë°ì´í„°ì—ì„œ íšŒì‚¬ëª… í™•ì¸"""
        metadata_text = " ".join([str(v) for v in metadata.values()]).lower()

        for variation in variations:
            if variation.lower() in metadata_text:
                return True
        return False

    def _check_company_in_content(self, content: str, variations: List[str]) -> bool:
        """ë¬¸ì„œ ë‚´ìš©ì—ì„œ íšŒì‚¬ëª… í™•ì¸"""
        content_lower = content.lower()

        for variation in variations:
            if variation.lower() in content_lower:
                return True
        return False

    def filter_by_section_type(
        self,
        documents: List[DocumentChunk],
        evaluation_type: EvaluationType
    ) -> List[DocumentChunk]:
        """í‰ê°€ ìœ í˜•ì— ë”°ë¥¸ ì„¹ì…˜ë³„ í•„í„°ë§"""

        section_keywords = {
            EvaluationType.FULL_EVALUATION: [],  # ëª¨ë“  ì„¹ì…˜
            EvaluationType.GROWTH_ANALYSIS: [
                "ì„±ì¥", "ë§¤ì¶œ", "ì‹œì¥ì ìœ ìœ¨", "í™•ì¥", "ì„±ì¥ë¥ ", "ì¦ê°€ìœ¨",
                "growth", "revenue", "expansion", "scale"
            ],
            EvaluationType.FINANCIAL_ANALYSIS: [
                "ì¬ë¬´", "ì†ìµ", "ìì‚°", "ë¶€ì±„", "í˜„ê¸ˆ", "íˆ¬ì", "ìˆ˜ìµ",
                "financial", "profit", "asset", "debt", "cash", "investment"
            ],
            EvaluationType.TECH_ANALYSIS: [
                "ê¸°ìˆ ", "ê°œë°œ", "íŠ¹í—ˆ", "R&D", "í˜ì‹ ", "í”Œë«í¼", "ì‹œìŠ¤í…œ",
                "technology", "development", "patent", "innovation", "platform"
            ],
            EvaluationType.RISK_ANALYSIS: [
                "ë¦¬ìŠ¤í¬", "ìœ„í—˜", "ê·œì œ", "ê²½ìŸ", "ìœ„ê¸°", "ë¬¸ì œ",
                "risk", "regulation", "competition", "crisis", "issue"
            ]
        }

        keywords = section_keywords.get(evaluation_type, [])
        if not keywords:  # ì „ì²´ í‰ê°€ì¸ ê²½ìš° ëª¨ë“  ë¬¸ì„œ ë°˜í™˜
            return documents

        filtered_docs = []
        for doc in documents:
            content_lower = doc.content.lower()
            if any(keyword.lower() in content_lower for keyword in keywords):
                filtered_docs.append(doc)

        return filtered_docs

    def rank_documents_by_relevance(
        self,
        documents: List[DocumentChunk],
        company_name: str,
        evaluation_type: EvaluationType,
        top_k: int = None
    ) -> List[DocumentChunk]:
        """ë¬¸ì„œë¥¼ ê´€ë ¨ì„±ìœ¼ë¡œ ìˆœìœ„ ë§¤ê¸°ê¸°"""
        if not documents:
            return []

        top_k = top_k or self.config["vector_db"].top_k_results

        # ê° ë¬¸ì„œì— ëŒ€í•´ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°
        scored_documents = []
        max_docs_for_llm = int(os.getenv("MAX_DOCS_FOR_LLM", "20"))
        for doc in documents[:max_docs_for_llm]:  # í™˜ê²½ë³€ìˆ˜ë¡œ ì„¤ì •ëœ ìƒìœ„ ë¬¸ì„œë§Œ
            try:
                relevance_data = self._evaluate_document_relevance(
                    doc, company_name, evaluation_type
                )
                doc.metadata.update(relevance_data)
                scored_documents.append(doc)
            except Exception as e:
                # LLM í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì ìˆ˜ ì‚¬ìš©
                doc.metadata["relevance_score"] = doc.similarity_score * 10
                scored_documents.append(doc)

        # ë‚˜ë¨¸ì§€ ë¬¸ì„œë“¤ì€ similarity_score ê¸°ë°˜ìœ¼ë¡œ ì¶”ê°€
        for doc in documents[max_docs_for_llm:]:
            doc.metadata["relevance_score"] = doc.similarity_score * 10
            scored_documents.append(doc)

        # ê´€ë ¨ì„± ì ìˆ˜ë¡œ ì •ë ¬
        scored_documents.sort(
            key=lambda x: x.metadata.get("relevance_score", 0),
            reverse=True
        )

        return scored_documents[:top_k]

    def _evaluate_document_relevance(
        self,
        document: DocumentChunk,
        company_name: str,
        evaluation_type: EvaluationType
    ) -> Dict[str, Any]:
        """LLMì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œ ê´€ë ¨ì„± í‰ê°€"""
        try:
            # ë¬¸ì„œ ë‚´ìš© ì œí•œ (í† í° ì œí•œ ê³ ë ¤)
            max_content_length = int(os.getenv("MAX_DOCUMENT_CONTENT_LENGTH", "1000"))
            content_preview = document.content[:max_content_length]

            response = self.llm.invoke(self.relevance_prompt.format(
                company_name=company_name,
                evaluation_type=evaluation_type.value,
                document_content=content_preview
            ))

            # GPT ì‘ë‹µì„ í„°ë¯¸ë„ì— ì¶œë ¥
            print(f"\nğŸ” DOCUMENT_RETRIEVAL_LAYER - GPT ì‘ë‹µ:")
            print("=" * 60)
            print(response.content)
            print("=" * 60)

            import json
            relevance_data = json.loads(response.content.strip())
            return relevance_data

        except Exception as e:
            # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
            return {
                "relevance_score": 5.0,
                "key_points": [],
                "section_type": "ê¸°íƒ€",
                "reasoning": "ìë™ í‰ê°€ ì‹¤íŒ¨"
            }

    def extract_top_k_chunks(
        self,
        documents: List[DocumentChunk],
        company_name: str,
        evaluation_type: EvaluationType,
        k: int = None
    ) -> List[DocumentChunk]:
        """ìµœì¢… Top-K ë¬¸ì„œ ì²­í¬ ì¶”ì¶œ"""
        if not documents:
            return []

        k = k or self.config["vector_db"].top_k_results

        # 1ë‹¨ê³„: íšŒì‚¬ëª…ìœ¼ë¡œ í•„í„°ë§
        company_filtered = self.filter_documents_by_company(documents, company_name)

        # 2ë‹¨ê³„: ì„¹ì…˜ íƒ€ì…ìœ¼ë¡œ í•„í„°ë§
        section_filtered = self.filter_by_section_type(company_filtered, evaluation_type)

        # í•„í„°ë§ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ì›ë³¸ ë¬¸ì„œì—ì„œ ìƒìœ„ ë¬¸ì„œ ì‚¬ìš©
        if not section_filtered:
            section_filtered = documents

        # 3ë‹¨ê³„: ê´€ë ¨ì„±ìœ¼ë¡œ ìˆœìœ„ ë§¤ê¸°ê³  Top-K ì„ íƒ
        ranked_documents = self.rank_documents_by_relevance(
            section_filtered, company_name, evaluation_type, k
        )

        return ranked_documents

def create_document_retrieval_layer() -> CompanyDocumentRetriever:
    """Document Retrieval Layer ìƒì„±ì"""
    return CompanyDocumentRetriever()

def process_document_retrieval_layer(context: PipelineContext) -> PipelineContext:
    """Document Retrieval Layer ì²˜ë¦¬ í•¨ìˆ˜"""
    retriever = create_document_retrieval_layer()

    # ê¸°ì¡´ ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì—ì„œ Top-K ì¶”ì¶œ
    if context.retrieved_documents:
        top_k_documents = retriever.extract_top_k_chunks(
            documents=context.retrieved_documents,
            company_name=context.company_info.name,
            evaluation_type=context.parsed_input.evaluation_type
        )

        # ì»¨í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸
        context.retrieved_documents = top_k_documents

        # ì²˜ë¦¬ ë‹¨ê³„ ê¸°ë¡
        context.processing_steps.append(
            f"DOCUMENT_RETRIEVAL_LAYER: {len(top_k_documents)}ê°œ Top-K ë¬¸ì„œ ì¶”ì¶œ ì™„ë£Œ"
        )
    else:
        context.processing_steps.append(
            "DOCUMENT_RETRIEVAL_LAYER: ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ìŒ"
        )

    return context