"""
Layer 9: QUALITY CHECK LAYER
relevance_checkerë¥¼ ì‹¤í–‰í•˜ì—¬ ë‹µë³€ì˜ ê´€ë ¨ì„±, ê·¼ê±° ì¶©ë¶„ì„±, ê°ê´€ì„±ì„ ê²€ì¦í•˜ëŠ” ë ˆì´ì–´
"""
from typing import List, Dict, Any, Optional
from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate
import os
import re
from datetime import datetime
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from models import (
    QualityCheckResult, InvestmentReport, AnalysisResult,
    RiskAssessment, DocumentChunk, ExternalSearchResult, PipelineContext, RiskLevel
)
from config import get_config

class RelevanceChecker:
    """ê´€ë ¨ì„± ê²€ì¦ê¸°"""

    def __init__(self):
        self.config = get_config()
        self.llm = ChatOpenAI(
            openai_api_key=self.config["model"].openai_api_key,
            temperature=0.1,
            model=self.config["model"].model_name  # ì˜ˆ: "gpt-4o-mini"
        )

        # ê´€ë ¨ì„± ê²€ì¦ í”„ë¡¬í”„íŠ¸
        self.relevance_check_prompt = PromptTemplate(
            input_variables=["company_name", "user_request", "report_content"],
            template="""ë‹¤ìŒ íˆ¬ì í‰ê°€ ë¦¬í¬íŠ¸ê°€ ì‚¬ìš©ì ìš”ì²­ì— ì–¼ë§ˆë‚˜ ê´€ë ¨ì„±ì´ ìˆëŠ”ì§€ í‰ê°€í•´ì£¼ì„¸ìš”.

íšŒì‚¬ëª…: {company_name}
ì‚¬ìš©ì ìš”ì²­: {user_request}

ìƒì„±ëœ ë¦¬í¬íŠ¸ ë‚´ìš©:
{report_content}

ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ 0-10ì  í‰ê°€í•´ì£¼ì„¸ìš”:
1. ìš”ì²­ëœ íšŒì‚¬ì— ëŒ€í•œ ì •í™•ì„±
2. ìš”ì²­ëœ í‰ê°€ ìœ í˜•ê³¼ì˜ ì¼ì¹˜ì„±
3. ë¶„ì„ ë‚´ìš©ì˜ êµ¬ì²´ì„±
4. ê²°ë¡ ì˜ ëª…í™•ì„±

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
{{
    "relevance_score": 8.5,
    "issues": ["ë¬¸ì œì  1", "ë¬¸ì œì  2"],
    "strengths": ["ê°•ì  1", "ê°•ì  2"]
}}"""
        )

    def check_relevance(
        self,
        company_name: str,
        user_request: str,
        report: InvestmentReport
    ) -> float:
        """ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°"""
        try:
            # ë¦¬í¬íŠ¸ ë‚´ìš© ìš”ì•½
            report_summary = f"""
            Executive Summary: {report.executive_summary[:300]}...
            ì´ì : {report.unicorn_score.total_score}
            ì¶”ì²œ: {report.recommendation.value}
            """

            response = self.llm.invoke(self.relevance_check_prompt.format(
                company_name=company_name,
                user_request=user_request,
                report_content=report_summary
            ))

            # GPT ì‘ë‹µì„ í„°ë¯¸ë„ì— ì¶œë ¥
            print(f"\nğŸ” QUALITY_CHECK_LAYER - GPT ì‘ë‹µ:")
            print("=" * 60)
            print(response.content)
            print("=" * 60)

            import json
            result_data = json.loads(response.content.strip())
            return result_data.get("relevance_score", 5.0) / 10.0

        except Exception as e:
            # ê¸°ë³¸ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°
            return self._calculate_basic_relevance(company_name, report)

    def _calculate_basic_relevance(self, company_name: str, report: InvestmentReport) -> float:
        """ê¸°ë³¸ ê´€ë ¨ì„± ê³„ì‚°"""
        score = 0.5  # ê¸°ë³¸ ì ìˆ˜

        # íšŒì‚¬ëª… ì¼ì¹˜ í™•ì¸
        if company_name.lower() in report.company_info.name.lower():
            score += 0.3

        # ë¶„ì„ ê²°ê³¼ ì¡´ì¬ í™•ì¸
        if report.analysis_results:
            score += 0.1

        # ë¦¬ìŠ¤í¬ í‰ê°€ ì¡´ì¬ í™•ì¸
        if report.risk_assessments:
            score += 0.1

        return min(score, 1.0)

class EvidenceQualityChecker:
    """ê·¼ê±° í’ˆì§ˆ ê²€ì¦ê¸°"""

    def __init__(self):
        self.config = get_config()
        self.llm = ChatOpenAI(
            openai_api_key=self.config["model"].openai_api_key,
            temperature=0.1,
            model=self.config["model"].model_name  # ì˜ˆ: "gpt-4o-mini"
        )

    def check_evidence_quality(
        self,
        analysis_results: List[AnalysisResult],
        documents: List[DocumentChunk],
        external_results: List[ExternalSearchResult]
    ) -> float:
        """ê·¼ê±° í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        quality_factors = []

        # 1. ë°ì´í„° ì†ŒìŠ¤ì˜ ë‹¤ì–‘ì„±
        source_diversity = self._calculate_source_diversity(documents, external_results)
        quality_factors.append(source_diversity)

        # 2. ê·¼ê±°ì˜ êµ¬ì²´ì„±
        evidence_specificity = self._calculate_evidence_specificity(analysis_results)
        quality_factors.append(evidence_specificity)

        # 3. ë°ì´í„°ì˜ ìµœì‹ ì„±
        data_freshness = self._calculate_data_freshness(external_results)
        quality_factors.append(data_freshness)

        # 4. ì •ëŸ‰ì  ê·¼ê±°ì˜ ë¹„ìœ¨
        quantitative_ratio = self._calculate_quantitative_ratio(analysis_results)
        quality_factors.append(quantitative_ratio)

        return sum(quality_factors) / len(quality_factors)

    def _calculate_source_diversity(
        self,
        documents: List[DocumentChunk],
        external_results: List[ExternalSearchResult]
    ) -> float:
        """ë°ì´í„° ì†ŒìŠ¤ ë‹¤ì–‘ì„± ê³„ì‚°"""
        sources = set()

        for doc in documents:
            sources.add(doc.source)

        for result in external_results:
            sources.add(result.source)

        # ì†ŒìŠ¤ ê°œìˆ˜ì— ë”°ë¥¸ ì ìˆ˜ (ìµœëŒ€ 1.0)
        return min(len(sources) / 10.0, 1.0)

    def _calculate_evidence_specificity(self, analysis_results: List[AnalysisResult]) -> float:
        """ê·¼ê±°ì˜ êµ¬ì²´ì„± ê³„ì‚°"""
        if not analysis_results:
            return 0.0

        total_evidence_items = 0
        for result in analysis_results:
            total_evidence_items += len(result.supporting_evidence)

        # ë¶„ì„ë‹¹ í‰ê·  ê·¼ê±° ê°œìˆ˜ (3ê°œ ì´ìƒì´ë©´ 1.0ì )
        avg_evidence = total_evidence_items / len(analysis_results)
        return min(avg_evidence / 3.0, 1.0)

    def _calculate_data_freshness(self, external_results: List[ExternalSearchResult]) -> float:
        """ë°ì´í„° ìµœì‹ ì„± ê³„ì‚°"""
        if not external_results:
            return 0.5

        recent_count = 0
        total_count = len(external_results)

        for result in external_results:
            if result.published_date:
                days_old = (datetime.now() - result.published_date).days
                if days_old <= 30:  # 30ì¼ ì´ë‚´
                    recent_count += 1

        return recent_count / total_count if total_count > 0 else 0.5

    def _calculate_quantitative_ratio(self, analysis_results: List[AnalysisResult]) -> float:
        """ì •ëŸ‰ì  ê·¼ê±° ë¹„ìœ¨ ê³„ì‚°"""
        if not analysis_results:
            return 0.0

        quantitative_count = 0
        total_evidence = 0

        for result in analysis_results:
            for evidence in result.supporting_evidence:
                total_evidence += 1
                # ìˆ«ìê°€ í¬í•¨ëœ ê·¼ê±°ì¸ì§€ í™•ì¸
                if re.search(r'\d+', evidence):
                    quantitative_count += 1

        return quantitative_count / total_evidence if total_evidence > 0 else 0.0

class ObjectivityChecker:
    """ê°ê´€ì„± ê²€ì¦ê¸°"""

    def __init__(self):
        self.config = get_config()
        self.llm = ChatOpenAI(
            openai_api_key=self.config["model"].openai_api_key,
            temperature=0.1,
            model=self.config["model"].model_name  # ì˜ˆ: "gpt-4o-mini"
        )

        self.objectivity_prompt = PromptTemplate(
            input_variables=["report_content"],
            template="""ë‹¤ìŒ íˆ¬ì í‰ê°€ ë¦¬í¬íŠ¸ì˜ ê°ê´€ì„±ì„ í‰ê°€í•´ì£¼ì„¸ìš”.

ë¦¬í¬íŠ¸ ë‚´ìš©:
{report_content}

ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ 0-10ì  í‰ê°€í•´ì£¼ì„¸ìš”:
1. í¸í–¥ë˜ì§€ ì•Šì€ ë¶„ì„
2. ì¥ì ê³¼ ë‹¨ì ì˜ ê· í˜•ì  ì œì‹œ
3. ê°ì •ì  í‘œí˜„ì˜ ë°°ì œ
4. ê·¼ê±° ê¸°ë°˜ ê²°ë¡  ë„ì¶œ
5. ë‹¤ì–‘í•œ ê´€ì ì˜ ê³ ë ¤

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
{{
    "objectivity_score": 7.5,
    "bias_indicators": ["í¸í–¥ ì§€í‘œ 1", "í¸í–¥ ì§€í‘œ 2"],
    "improvement_suggestions": ["ê°œì„  ì œì•ˆ 1", "ê°œì„  ì œì•ˆ 2"]
}}"""
        )

    def check_objectivity(self, report: InvestmentReport) -> Dict[str, Any]:
        """ê°ê´€ì„± ì ìˆ˜ ê³„ì‚°"""
        try:
            # ë¦¬í¬íŠ¸ ì£¼ìš” ë‚´ìš© ì¶”ì¶œ
            report_content = f"""
            Executive Summary: {report.executive_summary}
            Investment Rationale: {report.investment_rationale[:500]}
            """

            response = self.llm.invoke(self.objectivity_prompt.format(
                report_content=report_content
            ))

            # GPT ì‘ë‹µì„ í„°ë¯¸ë„ì— ì¶œë ¥
            print(f"\nğŸ” QUALITY_CHECK_LAYER (OBJECTIVITY) - GPT ì‘ë‹µ:")
            print("=" * 60)
            print(response.content)
            print("=" * 60)

            import json
            objectivity_data = json.loads(response.content.strip())

            return {
                "score": objectivity_data.get("objectivity_score", 5.0) / 10.0,
                "bias_indicators": objectivity_data.get("bias_indicators", []),
                "improvement_suggestions": objectivity_data.get("improvement_suggestions", [])
            }

        except Exception as e:
            return {
                "score": 0.7,  # ê¸°ë³¸ ê°ê´€ì„± ì ìˆ˜
                "bias_indicators": [],
                "improvement_suggestions": [f"ê°ê´€ì„± í‰ê°€ ì˜¤ë¥˜: {str(e)}"]
            }

class QualityChecker:
    """í’ˆì§ˆ ê²€ì¦ ë©”ì¸ í´ë˜ìŠ¤"""

    def __init__(self):
        self.relevance_checker = RelevanceChecker()
        self.evidence_checker = EvidenceQualityChecker()
        self.objectivity_checker = ObjectivityChecker()

    def perform_quality_check(
        self,
        report: InvestmentReport,
        original_request: str,
        documents: List[DocumentChunk],
        external_results: List[ExternalSearchResult]
    ) -> QualityCheckResult:
        """ì¢…í•©ì ì¸ í’ˆì§ˆ ê²€ì¦ ìˆ˜í–‰"""

        # 1. ê´€ë ¨ì„± ê²€ì¦
        relevance_score = self.relevance_checker.check_relevance(
            company_name=report.company_info.name,
            user_request=original_request,
            report=report
        )

        # 2. ê·¼ê±° í’ˆì§ˆ ê²€ì¦
        evidence_quality = self.evidence_checker.check_evidence_quality(
            analysis_results=report.analysis_results,
            documents=documents,
            external_results=external_results
        )

        # 3. ê°ê´€ì„± ê²€ì¦
        objectivity_data = self.objectivity_checker.check_objectivity(report)
        objectivity_score = objectivity_data["score"]

        # 4. ì „ì²´ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
        overall_quality = (relevance_score * 0.4 + evidence_quality * 0.3 + objectivity_score * 0.3)

        # 5. í’ˆì§ˆ ê²€ì¦ í†µê³¼ ì—¬ë¶€ ê²°ì •
        quality_threshold = float(os.getenv("QUALITY_THRESHOLD", "0.4"))  # ê¸°ë³¸ê°’ 40% ì´ìƒì´ë©´ í†µê³¼
        passed = overall_quality >= quality_threshold

        # 6. ì´ìŠˆ ë° ì œì•ˆì‚¬í•­ ìˆ˜ì§‘
        issues = []
        suggestions = []

        relevance_threshold = float(os.getenv("RELEVANCE_THRESHOLD", "0.7"))
        evidence_threshold = float(os.getenv("EVIDENCE_THRESHOLD", "0.6"))
        objectivity_threshold = float(os.getenv("OBJECTIVITY_THRESHOLD", "0.7"))

        if relevance_score < relevance_threshold:
            issues.append("ê´€ë ¨ì„± ë¶€ì¡±")
            suggestions.append("ë” êµ¬ì²´ì ì¸ íšŒì‚¬ ì •ë³´ ìˆ˜ì§‘ í•„ìš”")

        if evidence_quality < evidence_threshold:
            issues.append("ê·¼ê±° í’ˆì§ˆ ë¶€ì¡±")
            suggestions.append("ë” ë§ì€ ë°ì´í„° ì†ŒìŠ¤ í™œìš© í•„ìš”")

        if objectivity_score < objectivity_threshold:
            issues.append("ê°ê´€ì„± ë¶€ì¡±")
            suggestions.extend(objectivity_data["improvement_suggestions"])

        # ì¶”ê°€ ì´ìŠˆ í™•ì¸
        additional_issues = self._check_additional_issues(report)
        issues.extend(additional_issues["issues"])
        suggestions.extend(additional_issues["suggestions"])

        return QualityCheckResult(
            relevance_score=relevance_score,
            evidence_quality=evidence_quality,
            objectivity_score=objectivity_score,
            overall_quality=overall_quality,
            passed=passed,
            issues=issues,
            suggestions=suggestions
        )

    def _check_additional_issues(self, report: InvestmentReport) -> Dict[str, List[str]]:
        """ì¶”ê°€ í’ˆì§ˆ ì´ìŠˆ í™•ì¸"""
        issues = []
        suggestions = []

        # ë¶„ì„ ê²°ê³¼ ì™„ì„±ë„ í™•ì¸
        if len(report.analysis_results) < 4:
            issues.append("ë¶„ì„ ì˜ì—­ ë¶€ì¡±")
            suggestions.append("ëª¨ë“  ë¶„ì„ ì˜ì—­ ì™„ë£Œ í•„ìš”")

        # ë¦¬ìŠ¤í¬ í‰ê°€ ì™„ì„±ë„ í™•ì¸
        if len(report.risk_assessments) < 4:
            issues.append("ë¦¬ìŠ¤í¬ í‰ê°€ ë¶€ì¡±")
            suggestions.append("ì£¼ìš” ë¦¬ìŠ¤í¬ ì˜ì—­ í‰ê°€ í•„ìš”")

        # ì‹ ë¢°ë„ ìˆ˜ì¤€ í™•ì¸
        if report.confidence_level < 0.5:
            issues.append("ë‚®ì€ ë¶„ì„ ì‹ ë¢°ë„")
            suggestions.append("ì¶”ê°€ ë°ì´í„° ìˆ˜ì§‘ ë° ë¶„ì„ í•„ìš”")

        # ì ìˆ˜ ì¼ê´€ì„± í™•ì¸
        score_issues = self._check_score_consistency(report)
        issues.extend(score_issues["issues"])
        suggestions.extend(score_issues["suggestions"])

        return {"issues": issues, "suggestions": suggestions}

    def _check_score_consistency(self, report: InvestmentReport) -> Dict[str, List[str]]:
        """ì ìˆ˜ ì¼ê´€ì„± í™•ì¸"""
        issues = []
        suggestions = []

        # ì ìˆ˜ì™€ ë“±ê¸‰ ì¼ì¹˜ì„± í™•ì¸
        total_score = report.unicorn_score.total_score
        grade = report.unicorn_score.grade

        grade_ranges = {
            "S": (90, 100),
            "A": (80, 89),
            "B": (70, 79),
            "C": (60, 69),
            "D": (0, 59)
        }

        expected_range = grade_ranges.get(grade, (0, 100))
        if not (expected_range[0] <= total_score <= expected_range[1]):
            issues.append("ì ìˆ˜ì™€ ë“±ê¸‰ ë¶ˆì¼ì¹˜")
            suggestions.append("ì ìˆ˜ ê³„ì‚° ë¡œì§ ì¬ê²€í†  í•„ìš”")

        # íˆ¬ì ì¶”ì²œê³¼ ì ìˆ˜ ì¼ê´€ì„± í™•ì¸
        recommendation = report.recommendation.value
        if recommendation == "íˆ¬ì ì¶”ì²œ" and total_score < 70:
            issues.append("ë‚®ì€ ì ìˆ˜ ëŒ€ë¹„ íˆ¬ì ì¶”ì²œ ë¶ˆì¼ì¹˜")
            suggestions.append("íˆ¬ì ì¶”ì²œ ë¡œì§ ì¬ê²€í†  í•„ìš”")

        if recommendation == "íšŒí”¼" and total_score > 80:
            issues.append("ë†’ì€ ì ìˆ˜ ëŒ€ë¹„ íˆ¬ì íšŒí”¼ ë¶ˆì¼ì¹˜")
            suggestions.append("ë¦¬ìŠ¤í¬ ìš”ì¸ ì¬í‰ê°€ í•„ìš”")

        # ìœ ë‹ˆì½˜ í™•ë¥ ê³¼ ì´ì  ì¼ê´€ì„± í™•ì¸
        unicorn_prob = report.unicorn_score.unicorn_probability
        if total_score > 85 and unicorn_prob < 0.5:
            issues.append("ë†’ì€ ì ìˆ˜ ëŒ€ë¹„ ë‚®ì€ ìœ ë‹ˆì½˜ í™•ë¥ ")
            suggestions.append("ìœ ë‹ˆì½˜ í™•ë¥  ê³„ì‚° ë¡œì§ ì¬ê²€í† ")

        return {"issues": issues, "suggestions": suggestions}

class QualityCheckLayer:
    """í’ˆì§ˆ ê²€ì¦ ë ˆì´ì–´ ë©”ì¸ í´ë˜ìŠ¤"""

    def __init__(self):
        self.quality_checker = QualityChecker()

    def perform_quality_check(
        self,
        report: InvestmentReport,
        original_request: str,
        documents: List[DocumentChunk],
        external_results: List[ExternalSearchResult]
    ) -> QualityCheckResult:
        """í’ˆì§ˆ ê²€ì¦ ì‹¤í–‰"""
        return self.quality_checker.perform_quality_check(
            report=report,
            original_request=original_request,
            documents=documents,
            external_results=external_results
        )

    def should_regenerate_report(self, quality_result: QualityCheckResult) -> bool:
        """ë¦¬í¬íŠ¸ ì¬ìƒì„± í•„ìš” ì—¬ë¶€ íŒë‹¨"""
        # ê¸°ë³¸ì ìœ¼ë¡œ passedê°€ Falseì´ë©´ ì¬ìƒì„±
        if not quality_result.passed:
            return True

        # ì‹¬ê°í•œ ì´ìŠˆê°€ ìˆëŠ” ê²½ìš° ì¬ìƒì„±
        critical_issues = [
            "ì ìˆ˜ì™€ ë“±ê¸‰ ë¶ˆì¼ì¹˜",
            "ë‚®ì€ ì ìˆ˜ ëŒ€ë¹„ íˆ¬ì ì¶”ì²œ ë¶ˆì¼ì¹˜",
            "ë†’ì€ ì ìˆ˜ ëŒ€ë¹„ íˆ¬ì íšŒí”¼ ë¶ˆì¼ì¹˜"
        ]

        for issue in quality_result.issues:
            if issue in critical_issues:
                return True

        return False

    def generate_quality_improvement_recommendations(
        self,
        quality_result: QualityCheckResult
    ) -> List[str]:
        """í’ˆì§ˆ ê°œì„  ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []

        if quality_result.relevance_score < 0.7:
            recommendations.append("ê´€ë ¨ì„± ê°œì„ ì„ ìœ„í•´ ë” êµ¬ì²´ì ì¸ íšŒì‚¬ ë°ì´í„° ìˆ˜ì§‘")

        if quality_result.evidence_quality < 0.6:
            recommendations.append("ë¶„ì„ ê·¼ê±° ë³´ê°•ì„ ìœ„í•´ ì¶”ê°€ ë°ì´í„° ì†ŒìŠ¤ í™œìš©")

        if quality_result.objectivity_score < 0.7:
            recommendations.append("ê°ê´€ì„± í–¥ìƒì„ ìœ„í•´ ë‹¤ì–‘í•œ ê´€ì  ê³ ë ¤")

        if quality_result.overall_quality < 0.6:
            recommendations.append("ì „ë°˜ì ì¸ ë¶„ì„ í’ˆì§ˆ í–¥ìƒ í•„ìš”")

        recommendations.extend(quality_result.suggestions)

        return list(set(recommendations))  # ì¤‘ë³µ ì œê±°

def create_quality_check_layer() -> QualityCheckLayer:
    """Quality Check Layer ìƒì„±ì"""
    return QualityCheckLayer()

def process_quality_check_layer(context: PipelineContext, original_request: str) -> PipelineContext:
    """Quality Check Layer ì²˜ë¦¬ í•¨ìˆ˜"""
    quality_layer = create_quality_check_layer()

    if not context.final_report:
        # ë¦¬í¬íŠ¸ê°€ ì—†ëŠ” ê²½ìš°
        context.quality_check = QualityCheckResult(
            relevance_score=0.0,
            evidence_quality=0.0,
            objectivity_score=0.0,
            overall_quality=0.0,
            passed=False,
            issues=["ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨"],
            suggestions=["íŒŒì´í”„ë¼ì¸ ì „ì²´ ì¬ì‹¤í–‰ í•„ìš”"]
        )
        context.processing_steps.append("QUALITY_CHECK_LAYER: ë¦¬í¬íŠ¸ ì—†ìŒ - í’ˆì§ˆ ê²€ì¦ ì‹¤íŒ¨")
        return context

    # í’ˆì§ˆ ê²€ì¦ ì‹¤í–‰
    quality_result = quality_layer.perform_quality_check(
        report=context.final_report,
        original_request=original_request,
        documents=context.retrieved_documents,
        external_results=context.external_search_results
    )

    context.quality_check = quality_result

    # ì¬ìƒì„± í•„ìš” ì—¬ë¶€ í™•ì¸
    needs_regeneration = quality_layer.should_regenerate_report(quality_result)

    # ì²˜ë¦¬ ë‹¨ê³„ ê¸°ë¡
    status = "í†µê³¼" if quality_result.passed else "ì‹¤íŒ¨"
    regeneration_note = " (ì¬ìƒì„± í•„ìš”)" if needs_regeneration else ""

    context.processing_steps.append(
        f"QUALITY_CHECK_LAYER: í’ˆì§ˆ ê²€ì¦ {status} "
        f"(ê´€ë ¨ì„±: {quality_result.relevance_score:.1%}, "
        f"ê·¼ê±°: {quality_result.evidence_quality:.1%}, "
        f"ê°ê´€ì„±: {quality_result.objectivity_score:.1%}){regeneration_note}"
    )

    return context