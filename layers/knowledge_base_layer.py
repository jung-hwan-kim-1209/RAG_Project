"""
Layer 2: KNOWLEDGE BASE LAYER
Vector DB(Chroma/FAISS)ì—ì„œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰í•˜ëŠ” ë ˆì´ì–´
"""
import os
import logging
from typing import List, Optional, Dict, Any
from pathlib import Path

from langchain_community.vectorstores import Chroma
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import PyPDFLoader, TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from huggingface_hub import InferenceClient
import numpy as np

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from models import DocumentChunk, PipelineContext
from config import get_config

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class HuggingFaceEmbeddings:
    """HuggingFace Inference APIë¥¼ ì‚¬ìš©í•œ ì„ë² ë”© í´ë˜ìŠ¤"""
    
    def __init__(self, model_name: str, api_key: str):
        self.model_name = model_name
        self.client = InferenceClient(
            provider="hf-inference",
            api_key=api_key,
        )
    
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """ë¬¸ì„œë“¤ì„ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜"""
        embeddings = []
        for text in texts:
            try:
                # BAAI/bge-m3 ëª¨ë¸ ì‚¬ìš©
                embedding = self.client.feature_extraction(text, model=self.model_name)
                embeddings.append(embedding.tolist() if hasattr(embedding, 'tolist') else embedding)
            except Exception as e:
                logger.error(f"ì„ë² ë”© ìƒì„± ì˜¤ë¥˜: {e}")
                # ì˜¤ë¥˜ ì‹œ 0 ë²¡í„° ë°˜í™˜ (BAAI/bge-m3ëŠ” 1024ì°¨ì›)
                embeddings.append([0.0] * 1024)
        return embeddings
    
    def embed_query(self, text: str) -> List[float]:
        """ë‹¨ì¼ ì¿¼ë¦¬ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜"""
        try:
            embedding = self.client.feature_extraction(text, model=self.model_name)
            return embedding.tolist() if hasattr(embedding, 'tolist') else embedding
        except Exception as e:
            logger.error(f"ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± ì˜¤ë¥˜: {e}")
            return [0.0] * 1024

class VectorDBManager:
    """Vector Database ê´€ë¦¬ í´ë˜ìŠ¤"""

    def __init__(self):
        self.config = get_config()
        hf_token = os.getenv("HF_TOKEN")
        if not hf_token:
            raise ValueError("HF_TOKEN í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        self.embeddings = HuggingFaceEmbeddings(
            model_name=self.config["vector_db"].embedding_model,
            api_key=hf_token
        )
        self.chroma_db = None
        self.faiss_db = None
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=int(os.getenv("TEXT_CHUNK_SIZE", "1000")),
            chunk_overlap=int(os.getenv("TEXT_CHUNK_OVERLAP", "200")),
            length_function=len
        )

    def initialize_chroma(self) -> None:
        """ChromaDB ì´ˆê¸°í™”"""
        try:
            persist_directory = self.config["vector_db"].chroma_persist_directory
            os.makedirs(persist_directory, exist_ok=True)

            # ChromaDBìš© ì»¤ìŠ¤í…€ ì„ë² ë”© í•¨ìˆ˜ ìƒì„±
            def chroma_embedding_function(texts):
                return self.embeddings.embed_documents(texts)

            self.chroma_db = Chroma(
                persist_directory=persist_directory,
                embedding_function=chroma_embedding_function,
                collection_name=self.config["vector_db"].collection_name
            )
            logger.info("ChromaDB ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logger.error(f"ChromaDB ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

    def initialize_faiss(self) -> None:
        """FAISS ì´ˆê¸°í™”"""
        try:
            index_path = self.config["vector_db"].faiss_index_path
            faiss_file = f"{index_path}.faiss"
            pkl_file = f"{index_path}.pkl"
            
            if os.path.exists(faiss_file) and os.path.exists(pkl_file):
                import faiss
                import pickle
                
                # FAISS ì¸ë±ìŠ¤ ë¡œë“œ
                index = faiss.read_index(faiss_file)
                
                # ë©”íƒ€ë°ì´í„° ë¡œë“œ
                with open(pkl_file, 'rb') as f:
                    data = pickle.load(f)
                
                self.faiss_db = {
                    "index": index,
                    "texts": data["texts"],
                    "metadatas": data["metadatas"]
                }
                logger.info("FAISS ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ")
            else:
                logger.info("FAISS ì¸ë±ìŠ¤ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŒ")
        except Exception as e:
            logger.error(f"FAISS ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

    def load_documents_from_directory(self, directory_path: str, file_types: List[str] = None) -> List[Dict[str, Any]]:
        """ë””ë ‰í† ë¦¬ì—ì„œ ë¬¸ì„œë“¤ì„ ë¡œë“œ"""
        if file_types is None:
            file_types = [".pdf", ".txt", ".md"]

        documents = []
        directory = Path(directory_path)

        if not directory.exists():
            logger.warning(f"ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {directory_path}")
            return documents

        for file_path in directory.rglob("*"):
            if file_path.suffix.lower() in file_types:
                try:
                    docs = self._load_single_document(str(file_path))
                    documents.extend(docs)
                except Exception as e:
                    logger.error(f"ë¬¸ì„œ ë¡œë“œ ì‹¤íŒ¨: {file_path}, ì˜¤ë¥˜: {e}")

        return documents

    def _load_single_document(self, file_path: str) -> List[Dict[str, Any]]:
        """ê°œë³„ ë¬¸ì„œ ë¡œë“œ"""
        file_extension = Path(file_path).suffix.lower()

        if file_extension == ".pdf":
            loader = PyPDFLoader(file_path)
        else:
            loader = TextLoader(file_path, encoding="utf-8")

        documents = loader.load()
        split_docs = self.text_splitter.split_documents(documents)

        result = []
        for doc in split_docs:
            result.append({
                "content": doc.page_content,
                "metadata": {
                    **doc.metadata,
                    "source": file_path,
                    "file_type": file_extension
                }
            })

        return result

    def add_documents_to_chroma(self, documents: List[Dict[str, Any]]) -> None:
        """ChromaDBì— ë¬¸ì„œ ì¶”ê°€"""
        if not self.chroma_db:
            self.initialize_chroma()

        try:
            texts = [doc["content"] for doc in documents]
            metadatas = [doc["metadata"] for doc in documents]

            self.chroma_db.add_texts(
                texts=texts,
                metadatas=metadatas
            )
            logger.info(f"ChromaDBì— {len(documents)}ê°œ ë¬¸ì„œ ì¶”ê°€ ì™„ë£Œ")
        except Exception as e:
            logger.error(f"ChromaDB ë¬¸ì„œ ì¶”ê°€ ì‹¤íŒ¨: {e}")

    def add_documents_to_faiss(self, documents: List[Dict[str, Any]]) -> None:
        """FAISSì— ë¬¸ì„œ ì¶”ê°€"""
        try:
            texts = [doc["content"] for doc in documents]
            metadatas = [doc["metadata"] for doc in documents]
            
            # HuggingFace ì„ë² ë”©ìœ¼ë¡œ ë²¡í„° ìƒì„±
            embeddings = self.embeddings.embed_documents(texts)
            embeddings_array = np.array(embeddings).astype('float32')

            if self.faiss_db is None:
                # FAISS ì¸ë±ìŠ¤ ì§ì ‘ ìƒì„±
                import faiss
                dimension = embeddings_array.shape[1]
                index = faiss.IndexFlatIP(dimension)  # Inner Product (cosine similarity)
                
                # ì •ê·œí™” (cosine similarityë¥¼ ìœ„í•´)
                faiss.normalize_L2(embeddings_array)
                index.add(embeddings_array)
                
                # FAISS ì¸ë±ìŠ¤ì™€ ë©”íƒ€ë°ì´í„°ë¥¼ ì €ì¥
                index_path = self.config["vector_db"].faiss_index_path
                os.makedirs(os.path.dirname(index_path), exist_ok=True)
                faiss.write_index(index, f"{index_path}.faiss")
                
                # ë©”íƒ€ë°ì´í„° ì €ì¥
                import pickle
                with open(f"{index_path}.pkl", 'wb') as f:
                    pickle.dump({"texts": texts, "metadatas": metadatas}, f)
                
                # self.faiss_dbì— ì €ì¥ (ê²€ìƒ‰ìš©)
                self.faiss_db = {"index": index, "texts": texts, "metadatas": metadatas}
            else:
                # ê¸°ì¡´ ì¸ë±ìŠ¤ì— ì¶”ê°€
                faiss.normalize_L2(embeddings_array)
                self.faiss_db["index"].add(embeddings_array)
                self.faiss_db["texts"].extend(texts)
                self.faiss_db["metadatas"].extend(metadatas)
                
                # ì—…ë°ì´íŠ¸ëœ ì¸ë±ìŠ¤ ì €ì¥
                index_path = self.config["vector_db"].faiss_index_path
                faiss.write_index(self.faiss_db["index"], f"{index_path}.faiss")
                
                import pickle
                with open(f"{index_path}.pkl", 'wb') as f:
                    pickle.dump({"texts": self.faiss_db["texts"], "metadatas": self.faiss_db["metadatas"]}, f)

            logger.info(f"FAISSì— {len(documents)}ê°œ ë¬¸ì„œ ì¶”ê°€ ì™„ë£Œ")
        except Exception as e:
            logger.error(f"FAISS ë¬¸ì„œ ì¶”ê°€ ì‹¤íŒ¨: {e}")

    def search_chroma(self, query: str, k: int = None, filter_dict: Dict[str, Any] = None) -> List[DocumentChunk]:
        """ChromaDBì—ì„œ ê²€ìƒ‰"""
        if not self.chroma_db:
            return []

        k = k or self.config["vector_db"].top_k_results

        try:
            # ChromaDBì˜ similarity_search_with_scoreëŠ” ì»¤ìŠ¤í…€ ì„ë² ë”© í•¨ìˆ˜ì™€ í˜¸í™˜ë˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ
            # ëŒ€ì‹  ì§ì ‘ êµ¬í˜„
            query_embedding = self.embeddings.embed_query(query)
            
            # ChromaDBì—ì„œ ê²€ìƒ‰ (ì„ë² ë”© í•¨ìˆ˜ê°€ ìë™ìœ¼ë¡œ í˜¸ì¶œë¨)
            results = self.chroma_db.similarity_search_with_score(
                query=query,
                k=k,
                filter=filter_dict
            )

            chunks = []
            for doc, score in results:
                chunk = DocumentChunk(
                    content=doc.page_content,
                    source=doc.metadata.get("source", "unknown"),
                    metadata=doc.metadata,
                    similarity_score=score
                )
                chunks.append(chunk)

            return chunks
        except Exception as e:
            logger.error(f"ChromaDB ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            # ëŒ€ì•ˆìœ¼ë¡œ FAISS ê²€ìƒ‰ ì‚¬ìš©
            return self.search_faiss(query, k)

    def search_faiss(self, query: str, k: int = None) -> List[DocumentChunk]:
        """FAISSì—ì„œ ê²€ìƒ‰"""
        if not self.faiss_db:
            return []

        k = k or self.config["vector_db"].top_k_results

        try:
            # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
            query_embedding = self.embeddings.embed_query(query)
            query_array = np.array([query_embedding]).astype('float32')
            
            # ì •ê·œí™”
            import faiss
            faiss.normalize_L2(query_array)
            
            # ê²€ìƒ‰ ì‹¤í–‰
            scores, indices = self.faiss_db["index"].search(query_array, k)
            
            chunks = []
            for i, (score, idx) in enumerate(zip(scores[0], indices[0])):
                if idx >= 0 and idx < len(self.faiss_db["texts"]):
                    chunk = DocumentChunk(
                        content=self.faiss_db["texts"][idx],
                        source=self.faiss_db["metadatas"][idx].get("source", "unknown"),
                        metadata=self.faiss_db["metadatas"][idx],
                        similarity_score=float(score)
                    )
                    chunks.append(chunk)

            return chunks
        except Exception as e:
            logger.error(f"FAISS ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []

    def setup_initial_database(self) -> None:
        """ì´ˆê¸° ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •"""
        logger.info("ì´ˆê¸° ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì • ì‹œì‘")

        # ê° ë¬¸ì„œ íƒ€ì…ë³„ ë””ë ‰í† ë¦¬ì—ì„œ ë¬¸ì„œ ë¡œë“œ
        document_paths = self.config["document_paths"]

        # IR ë¬¸ì„œë“¤
        ir_docs = self.load_documents_from_directory(document_paths.ir_documents)
        if ir_docs:
            self.add_documents_to_chroma(ir_docs)
            self.add_documents_to_faiss(ir_docs)

        # ì‹œì¥ ë³´ê³ ì„œ
        market_docs = self.load_documents_from_directory(document_paths.market_reports)
        if market_docs:
            self.add_documents_to_chroma(market_docs)
            self.add_documents_to_faiss(market_docs)

        # íšŒì‚¬ í”„ë¡œí•„
        company_docs = self.load_documents_from_directory(document_paths.company_profiles)
        if company_docs:
            self.add_documents_to_chroma(company_docs)
            self.add_documents_to_faiss(company_docs)

        # ì¬ë¬´ ë¬¸ì„œ
        financial_docs = self.load_documents_from_directory(document_paths.financial_statements)
        if financial_docs:
            self.add_documents_to_chroma(financial_docs)
            self.add_documents_to_faiss(financial_docs)

        logger.info("ì´ˆê¸° ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì • ì™„ë£Œ")

class KnowledgeBase:
    """ì§€ì‹ ë² ì´ìŠ¤ ë©”ì¸ í´ë˜ìŠ¤"""

    def __init__(self):
        self.vector_db_manager = VectorDBManager()
        self.vector_db_manager.initialize_chroma()
        self.vector_db_manager.initialize_faiss()

    def search_knowledge_base(
        self,
        query: str,
        company_name: str = "",
        use_chroma: bool = True,
        use_faiss: bool = True,
        k: int = None
    ) -> List[DocumentChunk]:
        """ì§€ì‹ ë² ì´ìŠ¤ì—ì„œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰"""

        all_chunks = []

        # ChromaDB ê²€ìƒ‰
        if use_chroma:
            filter_dict = {}
            if company_name:
                filter_dict["company"] = company_name

            chroma_chunks = self.vector_db_manager.search_chroma(
                query=query,
                k=k,
                filter_dict=filter_dict if filter_dict else None
            )
            all_chunks.extend(chroma_chunks)

        # FAISS ê²€ìƒ‰
        if use_faiss:
            faiss_chunks = self.vector_db_manager.search_faiss(query=query, k=k)
            all_chunks.extend(faiss_chunks)

        # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
        unique_chunks = self._deduplicate_chunks(all_chunks)
        sorted_chunks = sorted(unique_chunks, key=lambda x: x.similarity_score, reverse=True)

        # ìƒìœ„ kê°œ ë°˜í™˜
        final_k = k or self.vector_db_manager.config["vector_db"].top_k_results
        return sorted_chunks[:final_k]

    def _deduplicate_chunks(self, chunks: List[DocumentChunk]) -> List[DocumentChunk]:
        """ì¤‘ë³µ ì²­í¬ ì œê±°"""
        seen_content = set()
        unique_chunks = []

        for chunk in chunks:
            content_hash = hash(chunk.content[:100])  # ì²« 100ìë¡œ í•´ì‹œ ìƒì„±
            if content_hash not in seen_content:
                seen_content.add(content_hash)
                unique_chunks.append(chunk)

        return unique_chunks

    def setup_database(self) -> None:
        """ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸° ì„¤ì •"""
        self.vector_db_manager.setup_initial_database()

def create_knowledge_base_layer() -> KnowledgeBase:
    """Knowledge Base Layer ìƒì„±ì"""
    return KnowledgeBase()

def process_knowledge_base_layer(context: PipelineContext) -> PipelineContext:
    """Knowledge Base Layer ì²˜ë¦¬ í•¨ìˆ˜"""
    knowledge_base = create_knowledge_base_layer()

    # ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
    company_name = context.company_info.name
    evaluation_type = context.parsed_input.evaluation_type.value

    search_query = f"{company_name} {evaluation_type} íˆ¬ì í‰ê°€ ë¶„ì„"

    # ì§€ì‹ ë² ì´ìŠ¤ì—ì„œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
    retrieved_chunks = knowledge_base.search_knowledge_base(
        query=search_query,
        company_name=company_name
    )

    context.retrieved_documents = retrieved_chunks

    # ì²˜ë¦¬ ë‹¨ê³„ ê¸°ë¡
    context.processing_steps.append(
        f"KNOWLEDGE_BASE_LAYER: {len(retrieved_chunks)}ê°œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ"
    )

    return context

# í…ŒìŠ¤íŠ¸ ì½”ë“œ
if __name__ == "__main__":
    print("ğŸ§ª Knowledge Base Layer í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    try:
        # VectorDBManager í…ŒìŠ¤íŠ¸
        print("1. VectorDBManager ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸...")
        manager = VectorDBManager()
        print("âœ… VectorDBManager ì´ˆê¸°í™” ì„±ê³µ")
        
        # HuggingFace ì„ë² ë”© í…ŒìŠ¤íŠ¸
        print("2. HuggingFace ì„ë² ë”© í…ŒìŠ¤íŠ¸...")
        test_text = "í…ŒìŠ¤íŠ¸ ë¬¸ì„œì…ë‹ˆë‹¤."
        embedding = manager.embeddings.embed_query(test_text)
        print(f"âœ… ì„ë² ë”© ìƒì„± ì„±ê³µ: {len(embedding)}ì°¨ì›")
        
        # KnowledgeBase í…ŒìŠ¤íŠ¸
        print("3. KnowledgeBase ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸...")
        kb = KnowledgeBase()
        print("âœ… KnowledgeBase ì´ˆê¸°í™” ì„±ê³µ")
        
        print("\nğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼!")
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()