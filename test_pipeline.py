"""
AI ìŠ¤íƒ€íŠ¸ì—… íˆ¬ìž í‰ê°€ ì—ì´ì „íŠ¸ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸
"""
import os
import unittest
from unittest.mock import Mock, patch
from datetime import datetime

# í…ŒìŠ¤íŠ¸ìš© í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
os.environ["OPENAI_API_KEY"] = "test_key"

from models import (
    CompanyInfo, ParsedInput, EvaluationType, DocumentChunk,
    ExternalSearchResult, AnalysisResult, RiskAssessment, RiskLevel,
    UnicornScore, InvestmentReport, InvestmentRecommendation, PipelineContext
)
from pipeline import InvestmentEvaluationPipeline
from layers.input_layer import InputParser
from layers.analysis_engine import GrowthAnalyzer
from layers.scoring_engine import UnicornScoreCalculator

class TestModels(unittest.TestCase):
    """ë°ì´í„° ëª¨ë¸ í…ŒìŠ¤íŠ¸"""

    def test_company_info_creation(self):
        """CompanyInfo ìƒì„± í…ŒìŠ¤íŠ¸"""
        company = CompanyInfo(
            name="í† ìŠ¤",
            industry="í•€í…Œí¬",
            founded_year=2013,
            headquarters="ì„œìš¸"
        )
        self.assertEqual(company.name, "í† ìŠ¤")
        self.assertEqual(company.industry, "í•€í…Œí¬")

    def test_analysis_result_creation(self):
        """AnalysisResult ìƒì„± í…ŒìŠ¤íŠ¸"""
        result = AnalysisResult(
            category="growth_analysis",
            score=85.0,
            grade="A",
            summary="ìš°ìˆ˜í•œ ì„±ìž¥ì„±",
            detailed_analysis="ìƒì„¸ ë¶„ì„ ë‚´ìš©",
            key_strengths=["ê°•ë ¥í•œ ì„±ìž¥", "ì‹œìž¥ í™•ìž¥"],
            key_weaknesses=["ê²½ìŸ ì‹¬í™”"]
        )
        self.assertEqual(result.score, 85.0)
        self.assertEqual(result.grade, "A")

    def test_risk_assessment_creation(self):
        """RiskAssessment ìƒì„± í…ŒìŠ¤íŠ¸"""
        risk = RiskAssessment(
            category="market_risk",
            risk_level=RiskLevel.MEDIUM,
            description="ì‹œìž¥ ìœ„í—˜ ì¤‘ê°„ ìˆ˜ì¤€",
            impact_score=6.0,
            probability=0.4
        )
        self.assertEqual(risk.risk_level, RiskLevel.MEDIUM)
        self.assertEqual(risk.impact_score, 6.0)

class TestInputLayer(unittest.TestCase):
    """ìž…ë ¥ ë ˆì´ì–´ í…ŒìŠ¤íŠ¸"""

    def setUp(self):
        self.input_parser = InputParser()

    @patch('layers.input_layer.OpenAI')
    def test_simple_parsing(self, mock_openai):
        """ê°„ë‹¨í•œ ìž…ë ¥ íŒŒì‹± í…ŒìŠ¤íŠ¸"""
        # Mock LLM ì‘ë‹µ
        mock_llm_instance = Mock()
        mock_llm_instance.return_value = '{"company_name": "í† ìŠ¤", "evaluation_type": "ì „ì²´ í‰ê°€", "specific_focus_areas": [], "additional_requirements": ""}'
        mock_openai.return_value = mock_llm_instance

        result = self.input_parser.parse("í† ìŠ¤ì˜ íˆ¬ìž ê°€ì¹˜ë¥¼ í‰ê°€í•´ì¤˜")

        self.assertEqual(result.company_name, "í† ìŠ¤")
        self.assertEqual(result.evaluation_type, EvaluationType.FULL_EVALUATION)

    def test_fallback_parsing(self):
        """ë°±ì—… íŒŒì‹± ë¡œì§ í…ŒìŠ¤íŠ¸"""
        result = self.input_parser._fallback_parsing("ì¹´ì¹´ì˜¤ ì„±ìž¥ì„± ë¶„ì„í•´ì¤˜")

        self.assertEqual(result.company_name, "ì¹´ì¹´ì˜¤")
        self.assertEqual(result.evaluation_type, EvaluationType.GROWTH_ANALYSIS)

    def test_company_info_extraction(self):
        """íšŒì‚¬ ì •ë³´ ì¶”ì¶œ í…ŒìŠ¤íŠ¸"""
        company_info = self.input_parser.extract_company_info("í† ìŠ¤")

        self.assertEqual(company_info.name, "í† ìŠ¤")
        # ì•Œë ¤ì§„ ê¸°ì—…ì¸ ê²½ìš° ì¶”ê°€ ì •ë³´ í™•ì¸
        if company_info.industry:
            self.assertEqual(company_info.industry, "í•€í…Œí¬")

class TestScoringEngine(unittest.TestCase):
    """ì ìˆ˜ ì—”ì§„ í…ŒìŠ¤íŠ¸"""

    def setUp(self):
        self.calculator = UnicornScoreCalculator()

    def test_grade_calculation(self):
        """ë“±ê¸‰ ê³„ì‚° í…ŒìŠ¤íŠ¸"""
        self.assertEqual(self.calculator.calculate_grade(95.0), "S")
        self.assertEqual(self.calculator.calculate_grade(85.0), "A")
        self.assertEqual(self.calculator.calculate_grade(75.0), "B")
        self.assertEqual(self.calculator.calculate_grade(65.0), "C")
        self.assertEqual(self.calculator.calculate_grade(45.0), "D")

    def test_weighted_score_calculation(self):
        """ê°€ì¤‘ì¹˜ ì ìˆ˜ ê³„ì‚° í…ŒìŠ¤íŠ¸"""
        analysis_results = [
            AnalysisResult("growth_analysis", 80.0, "B", "", "", [], []),
            AnalysisResult("business_model_analysis", 85.0, "A", "", "", [], []),
            AnalysisResult("tech_security_analysis", 90.0, "A", "", "", [], [])
        ]

        category_scores, weighted_scores = self.calculator.calculate_weighted_score(analysis_results)

        self.assertEqual(category_scores["growth_analysis"], 80.0)
        self.assertTrue(weighted_scores["growth_analysis"] > 0)

class TestPipeline(unittest.TestCase):
    """ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸"""

    def setUp(self):
        self.pipeline = InvestmentEvaluationPipeline()

    @patch('layers.input_layer.OpenAI')
    @patch('layers.knowledge_base_layer.Chroma')
    @patch('layers.external_search_layer.WebSearchAgent')
    def test_partial_pipeline_execution(self, mock_search, mock_chroma, mock_openai):
        """ë¶€ë¶„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ í…ŒìŠ¤íŠ¸"""
        # Mock ì„¤ì •
        mock_llm = Mock()
        mock_llm.return_value = '{"company_name": "í† ìŠ¤", "evaluation_type": "ì „ì²´ í‰ê°€", "specific_focus_areas": [], "additional_requirements": ""}'
        mock_openai.return_value = mock_llm

        # í…ŒìŠ¤íŠ¸ìš© ì»¨í…ìŠ¤íŠ¸ ì‹¤í–‰
        context = self.pipeline.execute_partial_pipeline(
            user_input="í† ìŠ¤ íˆ¬ìž í‰ê°€",
            start_layer="INPUT_LAYER",
            end_layer="INPUT_LAYER"
        )

        self.assertIsNotNone(context.parsed_input)
        self.assertTrue(len(context.processing_steps) > 0)

class TestQualityMetrics(unittest.TestCase):
    """í’ˆì§ˆ ì§€í‘œ í…ŒìŠ¤íŠ¸"""

    def test_relevance_calculation(self):
        """ê´€ë ¨ì„± ê³„ì‚° í…ŒìŠ¤íŠ¸"""
        from layers.quality_check_layer import RelevanceChecker

        checker = RelevanceChecker()

        # ê¸°ë³¸ ê´€ë ¨ì„± ê³„ì‚° í…ŒìŠ¤íŠ¸
        company_name = "í† ìŠ¤"
        report = Mock()
        report.company_info.name = "í† ìŠ¤"
        report.analysis_results = [Mock()]
        report.risk_assessments = [Mock()]

        relevance = checker._calculate_basic_relevance(company_name, report)

        self.assertTrue(0.0 <= relevance <= 1.0)
        self.assertGreater(relevance, 0.5)  # íšŒì‚¬ëª… ì¼ì¹˜ë¡œ ì¸í•œ ì ìˆ˜

class TestEndToEndScenarios(unittest.TestCase):
    """End-to-End ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸"""

    def setUp(self):
        self.test_queries = [
            "í† ìŠ¤ì˜ íˆ¬ìž ê°€ì¹˜ë¥¼ í‰ê°€í•´ì¤˜",
            "ì¹´ì¹´ì˜¤ ì„±ìž¥ì„± ë¶„ì„",
            "ë°°ë‹¬ì˜ë¯¼ì¡± ë¦¬ìŠ¤í¬ í‰ê°€",
            "ì¿ íŒ¡ ê¸°ìˆ ë ¥ ë¶„ì„"
        ]

    def test_query_parsing_scenarios(self):
        """ë‹¤ì–‘í•œ ì¿¼ë¦¬ íŒŒì‹± ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸"""
        parser = InputParser()

        for query in self.test_queries:
            with self.subTest(query=query):
                try:
                    result = parser._fallback_parsing(query)
                    self.assertIsNotNone(result.company_name)
                    self.assertIn(result.evaluation_type, list(EvaluationType))
                except Exception as e:
                    self.fail(f"Query parsing failed for '{query}': {e}")

class TestErrorHandling(unittest.TestCase):
    """ì—ëŸ¬ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""

    def test_missing_api_key(self):
        """API í‚¤ ëˆ„ë½ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
        # í™˜ê²½ ë³€ìˆ˜ì—ì„œ API í‚¤ ì œê±°
        original_key = os.environ.get("OPENAI_API_KEY")
        if "OPENAI_API_KEY" in os.environ:
            del os.environ["OPENAI_API_KEY"]

        try:
            from config import ModelConfig
            config = ModelConfig()
            self.assertEqual(config.openai_api_key, "")
        finally:
            # API í‚¤ ë³µì›
            if original_key:
                os.environ["OPENAI_API_KEY"] = original_key

    def test_empty_input_handling(self):
        """ë¹ˆ ìž…ë ¥ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
        parser = InputParser()
        result = parser._fallback_parsing("")

        self.assertEqual(result.company_name, "")
        self.assertEqual(result.evaluation_type, EvaluationType.FULL_EVALUATION)

class TestIntegration(unittest.TestCase):
    """í†µí•© í…ŒìŠ¤íŠ¸"""

    def test_mock_pipeline_flow(self):
        """Mockì„ ì‚¬ìš©í•œ íŒŒì´í”„ë¼ì¸ í”Œë¡œìš° í…ŒìŠ¤íŠ¸"""
        # í…ŒìŠ¤íŠ¸ìš© ì»¨í…ìŠ¤íŠ¸ ìƒì„±
        context = PipelineContext(
            parsed_input=ParsedInput(
                company_name="í† ìŠ¤",
                evaluation_type=EvaluationType.FULL_EVALUATION
            ),
            company_info=CompanyInfo(name="í† ìŠ¤", industry="í•€í…Œí¬"),
            retrieved_documents=[
                DocumentChunk("í…ŒìŠ¤íŠ¸ ë¬¸ì„œ ë‚´ìš©", "test_source.pdf")
            ],
            external_search_results=[
                ExternalSearchResult("í† ìŠ¤ ë‰´ìŠ¤", "í† ìŠ¤ ê´€ë ¨ ë‰´ìŠ¤ ë‚´ìš©", "news_source", "http://example.com")
            ],
            analysis_results=[
                AnalysisResult("growth_analysis", 85.0, "A", "ì„±ìž¥ì„± ìš°ìˆ˜", "ìƒì„¸ ë¶„ì„", ["ê°•ì 1"], ["ì•½ì 1"])
            ],
            risk_assessments=[
                RiskAssessment("market_risk", RiskLevel.LOW, "ì‹œìž¥ ë¦¬ìŠ¤í¬ ë‚®ìŒ", 3.0, 0.3)
            ]
        )

        # ê° ë‹¨ê³„ë³„ ë°ì´í„°ê°€ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸
        self.assertEqual(context.company_info.name, "í† ìŠ¤")
        self.assertEqual(len(context.analysis_results), 1)
        self.assertEqual(len(context.risk_assessments), 1)
        self.assertEqual(context.analysis_results[0].score, 85.0)

def create_test_suite():
    """í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸ ìƒì„±"""
    test_suite = unittest.TestSuite()

    # ê¸°ë³¸ í…ŒìŠ¤íŠ¸ë“¤
    test_suite.addTest(unittest.makeSuite(TestModels))
    test_suite.addTest(unittest.makeSuite(TestInputLayer))
    test_suite.addTest(unittest.makeSuite(TestScoringEngine))
    test_suite.addTest(unittest.makeSuite(TestQualityMetrics))
    test_suite.addTest(unittest.makeSuite(TestEndToEndScenarios))
    test_suite.addTest(unittest.makeSuite(TestErrorHandling))
    test_suite.addTest(unittest.makeSuite(TestIntegration))

    # íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ (ì‹œê°„ì´ ì˜¤ëž˜ ê±¸ë¦´ ìˆ˜ ìžˆì–´ ì„ íƒì ìœ¼ë¡œ)
    # test_suite.addTest(unittest.makeSuite(TestPipeline))

    return test_suite

def run_tests():
    """í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("ðŸ§ª AI ìŠ¤íƒ€íŠ¸ì—… íˆ¬ìž í‰ê°€ ì—ì´ì „íŠ¸ í…ŒìŠ¤íŠ¸ ì‹œìž‘...")
    print("=" * 60)

    test_suite = create_test_suite()
    runner = unittest.TextTestRunner(verbosity=2)
    result = runner.run(test_suite)

    print("=" * 60)
    print(f"ðŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
    print(f"   ì‹¤í–‰ëœ í…ŒìŠ¤íŠ¸: {result.testsRun}")
    print(f"   ì‹¤íŒ¨: {len(result.failures)}")
    print(f"   ì—ëŸ¬: {len(result.errors)}")

    if result.failures:
        print("\nâŒ ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸:")
        for test, traceback in result.failures:
            print(f"   - {test}: {traceback}")

    if result.errors:
        print("\nðŸ”¥ ì—ëŸ¬ê°€ ë°œìƒí•œ í…ŒìŠ¤íŠ¸:")
        for test, traceback in result.errors:
            print(f"   - {test}: {traceback}")

    success_rate = (result.testsRun - len(result.failures) - len(result.errors)) / result.testsRun
    print(f"\nâœ¨ ì„±ê³µë¥ : {success_rate:.1%}")

    return result.wasSuccessful()

if __name__ == '__main__':
    success = run_tests()
    exit(0 if success else 1)